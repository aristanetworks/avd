# Copyright (c) 2023 Arista Networks, Inc.
# Use of this source code is governed by the Apache License 2.0
# that can be found in the LICENSE file.
from __future__ import annotations

from functools import lru_cache
from pathlib import Path
from typing import TYPE_CHECKING

from yaml import SafeDumper, dump, safe_load

from ansible_collections.arista.avd.plugins.filter.natural_sort import natural_sort
from ansible_collections.arista.avd.plugins.plugin_utils.errors import AristaAvdError
from ansible_collections.arista.avd.plugins.plugin_utils.utils import get, get_item

if TYPE_CHECKING:
    from ansible_collections.arista.avd.plugins.plugin_utils.eos_designs_shared_utils import SharedUtils

FILE_HEADER = """\
# This file is auto-generated by AVD eos_designs.
# When there is a merge conflict for this file, rebase the branch, accept all current changes for this file, and re-run AVD.
"""


class AvdPoolManager:
    """
    Class used to handle pooled resources.

    This class is imported and initialized once in eos_designs_facts
    and given to shared_utils for each device.
    """

    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.id_files: set[Path] = set()
        self.changed_id_files: set[Path] = set()

    def _remove_stale_assignments(self) -> None:
        """
        Walk through all pools and in-place remove stale assignments and empty pools.

        An assigment is deemed stale if "accessed" is not True.
        The "accessed" field is also removed from the pool data.

        Updates self.changed_id_files if any objects were removed from self.id_pool_data.

        Note: This method is called from save_updated_pools and should _not_ be called manually.
        """
        for id_file in self.id_files:
            id_data = self.id_pool_data(id_file)
            for id_pool in id_data["id_pools"]:
                # Remove id_assignments that were not accesses using "last_accessed".
                len_before_removing_stale_entries = len(id_pool["id_assignments"])
                id_pool["id_assignments"] = [id_assignment for id_assignment in id_pool["id_assignments"] if id_assignment.pop("accessed", False)]
                if len_before_removing_stale_entries != len(id_pool["id_assignments"]):
                    self.changed_id_files.add(id_file)

            # Remove pools if there are no assignments left.
            id_pools_count = len(id_data["id_pools"])
            id_data["id_pools"] = [id_pool for id_pool in id_data["id_pools"] if id_pool["id_assignments"]]
            if id_pools_count != len(id_data["id_pools"]):
                self.changed_id_files.add(id_file)

    def save_updated_pools(self, Dumper=SafeDumper):
        """
        Save cached data for all changed files.

        Calls self.remove_stale_assignments first to ensure we save the cleaned data.

        Data is sorted to ensure a consistent layout.
        """
        self._remove_stale_assignments()
        for id_file in self.changed_id_files:
            id_data = self.id_pool_data(id_file)
            for id_pool in id_data["id_pools"]:
                # Sort id_assignments on "id".
                id_pool["id_assignments"] = natural_sort(id_pool["id_assignments"], "id")

            id_data["id_pools"] = natural_sort(id_data["id_pools"], "type")
            id_data["id_pools"] = natural_sort(id_data["id_pools"], "pod_name")
            id_data["id_pools"] = natural_sort(id_data["id_pools"], "dc_name")
            id_data["id_pools"] = natural_sort(id_data["id_pools"], "fabric_name")
            id_file.write_text(FILE_HEADER + dump(id_data, Dumper=Dumper), encoding="UTF-8")
        self.id_pool_data.cache_clear()
        self.changed_id_files = set()
        self.id_files = set()

    @lru_cache
    def id_pool_data(self, id_file: Path) -> dict:
        """
        Return the content of the given file.
        If the file does not exist, it will be created - including all directories.
        lru_cache ensures that we work with a cached copy of the data.
        Other functions will inplace update this cached data.

        Args:
            id_file: File to read id_pool_data from.
        """
        if not id_file.exists():
            # Try to create the dir and file.
            id_file.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
            id_file.touch(mode=0o664)

        # Read from file or assign an empty data model.
        id_data = safe_load(id_file.read_text(encoding="UTF-8")) or {"id_pools": []}

        if not isinstance(id_data, dict) or not isinstance(id_data.get("id_pools"), list):
            raise AristaAvdError(f"Invalid ID pool manager data when reading {id_file}. Expecting {'id_pools': []}")

        # Add the path to the list of all id_files. Used later for finding stale entries.
        self.id_files.add(id_file)

        return id_data

    def get_id(self, shared_utils: SharedUtils) -> int | None:
        """
        Returns the ID of one node if found in the pool.
        Otherwise a new entry is inserted into the pool and the ID is returned.

        The pools are dynamically built and matched on the following device data:
        - Fabric Name (from 'shared_utils.hostvars.fabric_name')
        - DC Name (from `shared_utils.dc_name`)
        - POD Name (from `shared_utils.pod_name`)
        - Node type (from `shared_utils.type`)

        Each pool will assign first available ID starting from 1.

        Args:
            shared_utils: Instance of SharedUtils initialized for one device.

        Returns:
            Node ID
        """
        fabric_name = get(shared_utils.hostvars, "fabric_name", required=True)
        default_id_file = f"{self.output_dir}/data/{fabric_name}-ids.yml"
        id_file = Path(get(shared_utils.hostvars, "fabric_numbering.id.pools_file", default=default_id_file))
        id_data = self.id_pool_data(id_file)

        id_pool = None
        for id_data_id_pool in id_data["id_pools"]:
            if (
                id_data_id_pool.get("fabric_name") == fabric_name
                and id_data_id_pool.get("dc_name") == shared_utils.dc_name
                and id_data_id_pool.get("pod_name") == shared_utils.pod_name
                and id_data_id_pool.get("type") == shared_utils.type
            ):
                id_pool = id_data_id_pool
                break

        if not id_pool:
            # Create a new dict, add to the list of pools.
            id_pool = {
                "fabric_name": fabric_name,
                "dc_name": shared_utils.dc_name,
                "pod_name": shared_utils.pod_name,
                "type": shared_utils.type,
                "id_assignments": [],
            }
            id_data["id_pools"].append(id_pool)

        if id_assignment := get_item(id_pool["id_assignments"], "hostname", shared_utils.hostname):
            # Found an ID, so return quickly.
            id_assignment["accessed"] = True
            return id_assignment["id"]

        existing_ids = set(map(lambda x: x.get("id"), id_pool["id_assignments"]))
        available_ids = set(range(1, len(existing_ids) + 2)) - existing_ids
        first_available_id = next(iter(available_ids))
        id_assignment = {
            "id": first_available_id,
            "hostname": shared_utils.hostname,
            "accessed": True,
        }
        id_pool["id_assignments"].append(id_assignment)

        # Since we assigned a new ID, we have to mark the id pool as changed to have it saved once the build is done.
        self.changed_id_files.add(id_file)
        return first_available_id
